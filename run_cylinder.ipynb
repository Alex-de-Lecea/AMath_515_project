{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dstre\\AppData\\Local\\Temp\\ipykernel_20612\\2077273905.py:7: UserWarning: Treat the new Tool classes introduced in v1.5 as experimental for now; the API and rcParam may change in future versions.\n",
      "  import matplotlib.pyplot as plt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import Autoencoder as AE\n",
    "import scipy\n",
    "import scipy.interpolate\n",
    "import os \n",
    "\n",
    "# use GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Reproducibility  \n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and Process Data ###\n",
    "run_interp = False \n",
    "cyl_data = np.load('./SamsStuff/data_numpy.npy') # of shape [n_samples, (u, v, vort), n_gridpoints]\n",
    "xy_coords = np.load('./SamsStuff/xy_coords.npy').T # of shape [n_gridpoints, (x, y)].T\n",
    "vort_data = cyl_data[:, 2, :]\n",
    "\n",
    "# interpolate onto structured grid\n",
    "x = xy_coords[0, :]\n",
    "y = xy_coords[1, :]\n",
    "x_bounds = [x.min(), x.max()]\n",
    "y_bounds = [y.min(), y.max()]\n",
    "scaling = 20 #controls resolution of structured grid\n",
    "Nx = int((x_bounds[1] - x_bounds[0]) * scaling)\n",
    "Ny = int((y_bounds[1] - y_bounds[0]) * scaling)\n",
    "grid_x, grid_y = np.meshgrid(np.linspace(x_bounds[0], x_bounds[1], Nx), np.linspace(y_bounds[0], y_bounds[1], Ny))\n",
    "\n",
    "# cylinder mask \n",
    "mask = np.ones_like(grid_x)\n",
    "mask[np.where(grid_x**2 + grid_y**2 < 0.5**2)] = 0\n",
    "\n",
    "if run_interp: # takes ~3m for scaling = 20\n",
    "    vort_structured = np.zeros((vort_data.shape[0], Ny, Nx))\n",
    "    for i in range(vort_data.shape[0]):\n",
    "        interpolated = scipy.interpolate.griddata((x,y), vort_data[i], (grid_x, grid_y), method='cubic')\n",
    "        vort_structured[i] = interpolated * mask\n",
    "    np.save('vort_structured{}.npy'.format(scaling), vort_structured)\n",
    "else:\n",
    "    vort_structured = np.load('vort_structured{}.npy'.format(scaling))\n",
    "\n",
    "im_size = vort_structured.shape[1] * vort_structured.shape[2]\n",
    "\n",
    "### Dataloaders ###\n",
    "test_perc = 0.1\n",
    "split_idx = int((1-test_perc) * vort_structured.shape[1])\n",
    "\n",
    "batch_size = 128\n",
    "vort_torch = torch.from_numpy(vort_structured).float()\n",
    "vort_dset = torch.utils.data.TensorDataset(vort_torch[split_idx:, :, :])\n",
    "dataloader = torch.utils.data.DataLoader(vort_dset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "vort_dset_test = torch.utils.data.TensorDataset(vort_torch[:split_idx, :, :])\n",
    "dataloader_test = torch.utils.data.DataLoader(vort_dset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with optimizer: Adagrad    lr = 0.01\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.14840935170650482\n",
      "Epoch: 6/200     Iter: 100     Loss: 2.005073070526123\n",
      "Epoch: 13/200     Iter: 200     Loss: 1.6002305746078491\n",
      "Epoch: 20/200     Iter: 300     Loss: 1.1154978275299072\n",
      "Epoch: 26/200     Iter: 400     Loss: 1.0599778890609741\n",
      "Epoch: 33/200     Iter: 500     Loss: 1.4518516063690186\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.6196743249893188\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.7234646081924438\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.2832223176956177\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.5216529369354248\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.5064589977264404\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.3058086335659027\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.3269287049770355\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.4564535617828369\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.24728219211101532\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.5250804424285889\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.453543096780777\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.1383761614561081\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.40308091044425964\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.39834320545196533\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.11165230721235275\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.21529169380664825\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.20009303092956543\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.10372310876846313\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.22381122410297394\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.2741924226284027\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.11158222705125809\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.3472987115383148\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.2233429104089737\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.12665732204914093\n",
      "\n",
      "Training with optimizer: Adagrad    lr = 0.001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.10761576890945435\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.05302827060222626\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.048099491745233536\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.04469282552599907\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.028891686350107193\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.02991137094795704\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.028886638581752777\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.021211912855505943\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.019746581092476845\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.021774163469672203\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.017081793397665024\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.0181909017264843\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.019719161093235016\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.013479448854923248\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.016233818605542183\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.015498924069106579\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.014582921750843525\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.012811634689569473\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.014587195590138435\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.01250245701521635\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.01166853029280901\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.012290490791201591\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.010120880790054798\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.011992811225354671\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.01280144788324833\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.008961544372141361\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.011490624397993088\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.012796756811439991\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.009223297238349915\n",
      "\n",
      "Training with optimizer: Adagrad    lr = 0.0001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.13776162266731262\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.1339985877275467\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.12982890009880066\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.12542055547237396\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.10737749934196472\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.10548881441354752\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.11351469159126282\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.1028134897351265\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.10239385068416595\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.1111392080783844\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.1016879752278328\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.1012231633067131\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.10996252298355103\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.10101538151502609\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.10046857595443726\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.1092604473233223\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.10055632144212723\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.0999419167637825\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.10872770845890045\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.10016284883022308\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.09951110929250717\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.1083243116736412\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.09984693676233292\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.0991487130522728\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.10799399018287659\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.09957434982061386\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.09883412718772888\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.1077188178896904\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.09932176023721695\n",
      "\n",
      "Training with optimizer: Adagrad    lr = 1e-05\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.1480996012687683\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.1459546834230423\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.1438441425561905\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.14554594457149506\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.14373591542243958\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.14189103245735168\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.14387337863445282\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.14212100207805634\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.14040331542491913\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.14258550107479095\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.1408456265926361\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.13922634720802307\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.14157605171203613\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.13983526825904846\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.13829351961612701\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.14078886806964874\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.13903649151325226\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.1375579535961151\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.1401783525943756\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.13840782642364502\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.13697843253612518\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.13970597088336945\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.13791100680828094\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.13651925325393677\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.13933725655078888\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.13751500844955444\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.13615290820598602\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.13904748857021332\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.13719616830348969\n",
      "\n",
      "Training with optimizer: SGD    lr = 0.01\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.15059754252433777\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.14956936240196228\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.14824961125850677\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.1504320204257965\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.1494068056344986\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.14809098839759827\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.15027910470962524\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.14925669133663177\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.14794443547725677\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.15013739466667175\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.14911754429340363\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.1478085219860077\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.150005504488945\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.14898811280727386\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.1476821005344391\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.14988277852535248\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.14886759221553802\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.14756445586681366\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.14976875483989716\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.14875558018684387\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.14745500683784485\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.14966252446174622\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.14865094423294067\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.1473526507616043\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.14956288039684296\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.1485527753829956\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.14725662767887115\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.14946942031383514\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.14846064150333405\n",
      "\n",
      "Training with optimizer: SGD    lr = 0.001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.15064996480941772\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.14967361092567444\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.14840415120124817\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.1506330519914627\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.14965660870075226\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.14838716387748718\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.15061625838279724\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.14963974058628082\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.14837028086185455\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.15059958398342133\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.14962299168109894\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.14835356175899506\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.15058302879333496\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.1496063619852066\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.14833691716194153\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.15056662261486053\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.14958986639976501\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.14832040667533875\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.15055032074451447\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.14957347512245178\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.1483040302991867\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.15053412318229675\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.1495572030544281\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.14828775823116302\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.15051805973052979\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.14954106509685516\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.1482716202735901\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.15050214529037476\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.14952506124973297\n",
      "\n",
      "Training with optimizer: SGD    lr = 0.0001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.1506551057100296\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.14968395233154297\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.14841967821121216\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.15065351128578186\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.14968234300613403\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.14841806888580322\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.1506519317626953\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.1496807485818863\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.1484164595603943\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.15065033733844757\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.14967913925647736\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.14841486513614655\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.15064875781536102\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.14967754483222961\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.1484132707118988\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.15064716339111328\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.14967595040798187\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.14841166138648987\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.15064558386802673\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.14967435598373413\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.14841006696224213\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.15064401924610138\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.1496727615594864\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.14840847253799438\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.15064242482185364\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.14967118203639984\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.14840687811374664\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.1506408452987671\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.1496695876121521\n",
      "\n",
      "Training with optimizer: SGD    lr = 1e-05\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.1506556123495102\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.14968493580818176\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.14842116832733154\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.15065550804138184\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.1496848315000534\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.14842106401920319\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.15065538883209229\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.14968474209308624\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.14842095971107483\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.15065529942512512\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.14968463778495789\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.14842085540294647\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.15065519511699677\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.14968451857566833\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.1484207659959793\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.1506550908088684\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.14968441426753998\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.14842064678668976\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.15065497159957886\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.14968432486057281\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.1484205573797226\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.1506548821926117\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.14968422055244446\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.14842043817043304\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.15065477788448334\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.1496841162443161\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.1484203338623047\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.15065468847751617\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.14968401193618774\n",
      "\n",
      "Training with optimizer: SGD_momentum    lr = 0.01\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.15017069876194\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.14877904951572418\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.14718352258205414\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.14915841817855835\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.14795204997062683\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.14649327099323273\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.14857922494411469\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.14744502305984497\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.1460423320531845\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.14817948639392853\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.14707088470458984\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.1456911265850067\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.1478564441204071\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.14675526320934296\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.14538513123989105\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.14756843447685242\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.14646649360656738\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.14510084688663483\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.1472979038953781\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.14619196951389313\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.14482906460762024\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.14703747630119324\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.14592541754245758\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.14456434547901154\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.14678356051445007\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.1456650048494339\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.14430604875087738\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.14653602242469788\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.14541040360927582\n",
      "\n",
      "Training with optimizer: SGD_momentum    lr = 0.001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.15060214698314667\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.14957384765148163\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.148253932595253\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.150436133146286\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.14941081404685974\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.14809487760066986\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.15028278529644012\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.1492602676153183\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.14794795215129852\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.15014071762561798\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.14912082254886627\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.14781171083450317\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.15000851452350616\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.14899109303951263\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.14768502116203308\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.14988552033901215\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.1488703042268753\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.14756710827350616\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.14977125823497772\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.14875808358192444\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.14745746552944183\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.14966481924057007\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.1486532837152481\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.14735493063926697\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.14956501126289368\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.1485549509525299\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.14725875854492188\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.14947140216827393\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.1484626829624176\n",
      "\n",
      "Training with optimizer: SGD_momentum    lr = 0.0001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.15065044164657593\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.14967408776283264\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.14840464293956757\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.1506335437297821\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.14965713024139404\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.14838768541812897\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.15061675012111664\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.1496402770280838\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.14837083220481873\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.15060009062290192\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.1496235430240631\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.14835412800312042\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.15058356523513794\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.14960695803165436\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.14833751320838928\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.1505671590566635\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.14959047734737396\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.1483210325241089\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.15055085718631744\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.14957408607006073\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.14830465614795685\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.15053468942642212\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.14955784380435944\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.14828839898109436\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.15051865577697754\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.1495417356491089\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.148272305727005\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.15050272643566132\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.1495257467031479\n",
      "\n",
      "Training with optimizer: SGD_momentum    lr = 1e-05\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.15065516531467438\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.14968401193618774\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.14841972291469574\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.15065358579158783\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.14968241751194\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.148418128490448\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.1506519913673401\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.14968082308769226\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.14841654896736145\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.15065041184425354\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.1496792435646057\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.1484149694442749\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.15064884722232819\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.14967766404151917\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.14841337502002716\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.15064726769924164\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.14967606961727142\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.1484117954969406\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.1506456732749939\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.14967449009418488\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.14841020107269287\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.15064410865306854\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.14967289566993713\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.14840862154960632\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.1506425440311432\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.14967133104801178\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.14840702712535858\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.15064096450805664\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.14966975152492523\n",
      "\n",
      "Training with optimizer: Adam    lr = 0.01\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 208624784.0\n",
      "Epoch: 13/200     Iter: 200     Loss: 1409528576.0\n",
      "Epoch: 20/200     Iter: 300     Loss: 4619131.0\n",
      "Epoch: 26/200     Iter: 400     Loss: 5795512.0\n",
      "Epoch: 33/200     Iter: 500     Loss: 3370602.0\n",
      "Epoch: 40/200     Iter: 600     Loss: 1891156.125\n",
      "Epoch: 46/200     Iter: 700     Loss: 3532220.75\n",
      "Epoch: 53/200     Iter: 800     Loss: 833053.875\n",
      "Epoch: 60/200     Iter: 900     Loss: 763247.0625\n",
      "Epoch: 66/200     Iter: 1000     Loss: 836265.0625\n",
      "Epoch: 73/200     Iter: 1100     Loss: 832455.375\n",
      "Epoch: 80/200     Iter: 1200     Loss: 428786.59375\n",
      "Epoch: 86/200     Iter: 1300     Loss: 1149468.125\n",
      "Epoch: 93/200     Iter: 1400     Loss: 927303.875\n",
      "Epoch: 100/200     Iter: 1500     Loss: 829695.375\n",
      "Epoch: 106/200     Iter: 1600     Loss: 557991.375\n",
      "Epoch: 113/200     Iter: 1700     Loss: 328755.28125\n",
      "Epoch: 120/200     Iter: 1800     Loss: 305701.5625\n",
      "Epoch: 126/200     Iter: 1900     Loss: 274190.0625\n",
      "Epoch: 133/200     Iter: 2000     Loss: 485841.90625\n",
      "Epoch: 140/200     Iter: 2100     Loss: 738461.5625\n",
      "Epoch: 146/200     Iter: 2200     Loss: 1884773.125\n",
      "Epoch: 153/200     Iter: 2300     Loss: 951770.5\n",
      "Epoch: 160/200     Iter: 2400     Loss: 1933184.625\n",
      "Epoch: 166/200     Iter: 2500     Loss: 528063.25\n",
      "Epoch: 173/200     Iter: 2600     Loss: 299022.90625\n",
      "Epoch: 180/200     Iter: 2700     Loss: 104407.3203125\n",
      "Epoch: 186/200     Iter: 2800     Loss: 2587594.5\n",
      "Epoch: 193/200     Iter: 2900     Loss: 554072.4375\n",
      "\n",
      "Training with optimizer: Adam    lr = 0.001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.11829672753810883\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.09195828437805176\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.10295934975147247\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.09952961653470993\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.06930890679359436\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.08915103226900101\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.1092248484492302\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.13355526328086853\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.08570075780153275\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.07530054450035095\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.06742794066667557\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.034731946885585785\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.05542078614234924\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.049656402319669724\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.01995948702096939\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.023938355967402458\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.02666480652987957\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.01630922593176365\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.017399318516254425\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.02141815423965454\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.011640598997473717\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.014982282184064388\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.026316814124584198\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.011292196810245514\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.017855001613497734\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.020292416214942932\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.009591727517545223\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.015293543227016926\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.019537951797246933\n",
      "\n",
      "Training with optimizer: Adam    lr = 0.0001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.12798132002353668\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.10055974125862122\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.09651447832584381\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.10751473158597946\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.062904953956604\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.07677813619375229\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.11132961511611938\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.05139963701367378\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.04513745754957199\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.06522669643163681\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.05557742342352867\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.028058858588337898\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.03288926184177399\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.04235522821545601\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.01927170716226101\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.021423274651169777\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.024747004732489586\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.01303753163665533\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.017368918284773827\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.0192286167293787\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.011596237309277058\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.018833758309483528\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.01949874497950077\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.00861915573477745\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.013997833244502544\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.014744956977665424\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.008275344967842102\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.0114018265157938\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.01693561114370823\n",
      "\n",
      "Training with optimizer: Adam    lr = 1e-05\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.14038698375225067\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.13554319739341736\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.13209545612335205\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.124177947640419\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.10426133871078491\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.10191433876752853\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.11037735641002655\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.10059066861867905\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.0995805636048317\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.10825096070766449\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.09914384037256241\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.09830670803785324\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.10723908245563507\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.09840765595436096\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.09764334559440613\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.10665235668420792\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.09799978882074356\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.09729979932308197\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.10637089610099792\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.09777539223432541\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.09708388149738312\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.10616403818130493\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.09758976846933365\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.09690538048744202\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.10601359605789185\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.09749020636081696\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.09683223813772202\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.1059432327747345\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.09745077788829803\n",
      "\n",
      "Training with optimizer: Adam_beta0.5    lr = 0.01\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 40.82594680786133\n",
      "Epoch: 13/200     Iter: 200     Loss: 13.940138816833496\n",
      "Epoch: 20/200     Iter: 300     Loss: 4.296127796173096\n",
      "Epoch: 26/200     Iter: 400     Loss: 7.099446773529053\n",
      "Epoch: 33/200     Iter: 500     Loss: 4.7828192710876465\n",
      "Epoch: 40/200     Iter: 600     Loss: 3.839460849761963\n",
      "Epoch: 46/200     Iter: 700     Loss: 2.404829740524292\n",
      "Epoch: 53/200     Iter: 800     Loss: 1.3818392753601074\n",
      "Epoch: 60/200     Iter: 900     Loss: 1.6520839929580688\n",
      "Epoch: 66/200     Iter: 1000     Loss: 1.941597580909729\n",
      "Epoch: 73/200     Iter: 1100     Loss: 9.806360244750977\n",
      "Epoch: 80/200     Iter: 1200     Loss: 5.7539496421813965\n",
      "Epoch: 86/200     Iter: 1300     Loss: 1.9045010805130005\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.6806800365447998\n",
      "Epoch: 100/200     Iter: 1500     Loss: 1.0745532512664795\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.4417288601398468\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.43521425127983093\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.4532197117805481\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.2811031639575958\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.295318603515625\n",
      "Epoch: 140/200     Iter: 2100     Loss: 1.114011526107788\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.8232131600379944\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.574456512928009\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.4636690318584442\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.323759526014328\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.34885069727897644\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.9848766326904297\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.20717619359493256\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.2368217259645462\n",
      "\n",
      "Training with optimizer: Adam_beta0.5    lr = 0.001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.12352702021598816\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.08778399229049683\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.09545358270406723\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.10961144417524338\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.09053630381822586\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.10604206472635269\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.0957157090306282\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.06575947999954224\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.08440107107162476\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.09608317911624908\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.06113355606794357\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.10665052384138107\n",
      "Epoch: 86/200     Iter: 1300     Loss: 184.6353759765625\n",
      "Epoch: 93/200     Iter: 1400     Loss: 61.42876052856445\n",
      "Epoch: 100/200     Iter: 1500     Loss: 25.106258392333984\n",
      "Epoch: 106/200     Iter: 1600     Loss: 40.64570999145508\n",
      "Epoch: 113/200     Iter: 1700     Loss: 36.25276565551758\n",
      "Epoch: 120/200     Iter: 1800     Loss: 16.09796714782715\n",
      "Epoch: 126/200     Iter: 1900     Loss: 13.482980728149414\n",
      "Epoch: 133/200     Iter: 2000     Loss: 10.101161003112793\n",
      "Epoch: 140/200     Iter: 2100     Loss: 11.835580825805664\n",
      "Epoch: 146/200     Iter: 2200     Loss: 20.648189544677734\n",
      "Epoch: 153/200     Iter: 2300     Loss: 4.973257064819336\n",
      "Epoch: 160/200     Iter: 2400     Loss: 7.205587387084961\n",
      "Epoch: 166/200     Iter: 2500     Loss: 14.244584083557129\n",
      "Epoch: 173/200     Iter: 2600     Loss: 9.251790046691895\n",
      "Epoch: 180/200     Iter: 2700     Loss: 6.751988887786865\n",
      "Epoch: 186/200     Iter: 2800     Loss: 8.235611915588379\n",
      "Epoch: 193/200     Iter: 2900     Loss: 4.75278902053833\n",
      "\n",
      "Training with optimizer: Adam_beta0.5    lr = 0.0001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.12621091306209564\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.08594898134469986\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.09345277398824692\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.10851309448480606\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.0614369735121727\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.08325880020856857\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.10831630229949951\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.061062026768922806\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.0626886636018753\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.06272215396165848\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.04972073435783386\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.03709276765584946\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.044637612998485565\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.03199747949838638\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.02802194096148014\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.04005720093846321\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.025259168818593025\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.02434648387134075\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.0281328447163105\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.022535687312483788\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.019165821373462677\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.02286423183977604\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.02072586864233017\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.01899813860654831\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.021706614643335342\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.01876329816877842\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.017233116552233696\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.023072462528944016\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.019120406359434128\n",
      "\n",
      "Training with optimizer: Adam_beta0.5    lr = 1e-05\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.13976356387138367\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.13514192402362823\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.13071584701538086\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.11903832852840424\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.10364974290132523\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.10187952220439911\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.11056327819824219\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.10136337578296661\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.09967903792858124\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.10887092351913452\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.10007236152887344\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.0985889881849289\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.10804519057273865\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.09947434812784195\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.09795747697353363\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.10752089321613312\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.09913891553878784\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.0976063460111618\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.10724269598722458\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.09890317171812057\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.09727296978235245\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.10692857205867767\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.09867316484451294\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.0971277579665184\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.10675322264432907\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.098561592400074\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.09700951725244522\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.10668804496526718\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.09855510294437408\n",
      "\n",
      "Training with optimizer: RMSprop    lr = 0.01\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 1418781392896.0\n",
      "Epoch: 13/200     Iter: 200     Loss: 1324826492928.0\n",
      "Epoch: 20/200     Iter: 300     Loss: 290531377152.0\n",
      "Epoch: 26/200     Iter: 400     Loss: 4586710499328.0\n",
      "Epoch: 33/200     Iter: 500     Loss: 67375538176.0\n",
      "Epoch: 40/200     Iter: 600     Loss: 31612874752.0\n",
      "Epoch: 46/200     Iter: 700     Loss: 1076803469312.0\n",
      "Epoch: 53/200     Iter: 800     Loss: 16113307648.0\n",
      "Epoch: 60/200     Iter: 900     Loss: 17701507072.0\n",
      "Epoch: 66/200     Iter: 1000     Loss: 33477076451328.0\n",
      "Epoch: 73/200     Iter: 1100     Loss: 204833259520.0\n",
      "Epoch: 80/200     Iter: 1200     Loss: 66998026240.0\n",
      "Epoch: 86/200     Iter: 1300     Loss: 71651409920.0\n",
      "Epoch: 93/200     Iter: 1400     Loss: 22663118848.0\n",
      "Epoch: 100/200     Iter: 1500     Loss: 27661103104.0\n",
      "Epoch: 106/200     Iter: 1600     Loss: 19086948352.0\n",
      "Epoch: 113/200     Iter: 1700     Loss: 4681765888.0\n",
      "Epoch: 120/200     Iter: 1800     Loss: 1819474560.0\n",
      "Epoch: 126/200     Iter: 1900     Loss: 1443956608.0\n",
      "Epoch: 133/200     Iter: 2000     Loss: 456467456.0\n",
      "Epoch: 140/200     Iter: 2100     Loss: 16416698368.0\n",
      "Epoch: 146/200     Iter: 2200     Loss: 5403630080.0\n",
      "Epoch: 153/200     Iter: 2300     Loss: 9430127616.0\n",
      "Epoch: 160/200     Iter: 2400     Loss: 11241452544.0\n",
      "Epoch: 166/200     Iter: 2500     Loss: 2188056832.0\n",
      "Epoch: 173/200     Iter: 2600     Loss: 3361104896.0\n",
      "Epoch: 180/200     Iter: 2700     Loss: 1490618752.0\n",
      "Epoch: 186/200     Iter: 2800     Loss: 296542272.0\n",
      "Epoch: 193/200     Iter: 2900     Loss: 188190032.0\n",
      "\n",
      "Training with optimizer: RMSprop    lr = 0.001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.7673642635345459\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.8730484247207642\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.7365955710411072\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.2517778277397156\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.7096207737922668\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.27917298674583435\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.20311276614665985\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.10929638147354126\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.14483845233917236\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.3333439230918884\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.09376858919858932\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.08917346596717834\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.08278752118349075\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.16706784069538116\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.10271557420492172\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.12071838974952698\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.1410648226737976\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.10099884867668152\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.0985126793384552\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.10160383582115173\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.12001258134841919\n",
      "Epoch: 146/200     Iter: 2200     Loss: 17.94239044189453\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.8731095194816589\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.34824123978614807\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.3630352318286896\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.14996393024921417\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.23167450726032257\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.19891610741615295\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.15686210989952087\n",
      "\n",
      "Training with optimizer: RMSprop    lr = 0.0001\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.1280314177274704\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.0699782446026802\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.09509830921888351\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.07683349400758743\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.04837343841791153\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.05507136136293411\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.04941597208380699\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.03191332891583443\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.036014482378959656\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.02606227993965149\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.030220916494727135\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.029666533693671227\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.028195854276418686\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.021652881056070328\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.026314040645956993\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.03254275023937225\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.023356936872005463\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.022968478500843048\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.02009899541735649\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.01836436800658703\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.022223256528377533\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.023603947833180428\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.022157417610287666\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.022489255294203758\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.018718959763646126\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.018195709213614464\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.019949600100517273\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.018427511677145958\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.019486648961901665\n",
      "\n",
      "Training with optimizer: RMSprop    lr = 1e-05\n",
      "Epoch: 0/200     Iter: 0     Loss: 0.1484212577342987\n",
      "Epoch: 6/200     Iter: 100     Loss: 0.13786786794662476\n",
      "Epoch: 13/200     Iter: 200     Loss: 0.13348278403282166\n",
      "Epoch: 20/200     Iter: 300     Loss: 0.12196195870637894\n",
      "Epoch: 26/200     Iter: 400     Loss: 0.11857184767723083\n",
      "Epoch: 33/200     Iter: 500     Loss: 0.10438107699155807\n",
      "Epoch: 40/200     Iter: 600     Loss: 0.10367482900619507\n",
      "Epoch: 46/200     Iter: 700     Loss: 0.11208485066890717\n",
      "Epoch: 53/200     Iter: 800     Loss: 0.10226906836032867\n",
      "Epoch: 60/200     Iter: 900     Loss: 0.10156718641519547\n",
      "Epoch: 66/200     Iter: 1000     Loss: 0.11043991148471832\n",
      "Epoch: 73/200     Iter: 1100     Loss: 0.10131777077913284\n",
      "Epoch: 80/200     Iter: 1200     Loss: 0.10014364123344421\n",
      "Epoch: 86/200     Iter: 1300     Loss: 0.10925585776567459\n",
      "Epoch: 93/200     Iter: 1400     Loss: 0.10061129182577133\n",
      "Epoch: 100/200     Iter: 1500     Loss: 0.09912313520908356\n",
      "Epoch: 106/200     Iter: 1600     Loss: 0.10869798064231873\n",
      "Epoch: 113/200     Iter: 1700     Loss: 0.10011592507362366\n",
      "Epoch: 120/200     Iter: 1800     Loss: 0.09890703856945038\n",
      "Epoch: 126/200     Iter: 1900     Loss: 0.10823168605566025\n",
      "Epoch: 133/200     Iter: 2000     Loss: 0.10009414702653885\n",
      "Epoch: 140/200     Iter: 2100     Loss: 0.09843607246875763\n",
      "Epoch: 146/200     Iter: 2200     Loss: 0.10799456387758255\n",
      "Epoch: 153/200     Iter: 2300     Loss: 0.09990154951810837\n",
      "Epoch: 160/200     Iter: 2400     Loss: 0.09834221005439758\n",
      "Epoch: 166/200     Iter: 2500     Loss: 0.10797521471977234\n",
      "Epoch: 173/200     Iter: 2600     Loss: 0.09944962710142136\n",
      "Epoch: 180/200     Iter: 2700     Loss: 0.09798742085695267\n",
      "Epoch: 186/200     Iter: 2800     Loss: 0.10743061453104019\n",
      "Epoch: 193/200     Iter: 2900     Loss: 0.09964624792337418\n"
     ]
    }
   ],
   "source": [
    "optimizers = [\n",
    "    'Adagrad',\n",
    "    'SGD',\n",
    "    'SGD_momentum',\n",
    "    'Adam',\n",
    "    'Adam_beta0.5',\n",
    "    'RMSprop',\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "lrs = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "EPOCHS = 200\n",
    "SAVE_DIR = './train_results_cyl/trial6/'\n",
    "\n",
    "val_losses = {}\n",
    "for opt in optimizers:\n",
    "    for lr in lrs:\n",
    "        #create directory for results\n",
    "        out_dir = SAVE_DIR + opt + '/'\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        print('\\nTraining with optimizer: {}    lr = {}'.format(opt, lr))\n",
    "        #Init network\n",
    "        n_latent = int(0.01* cyl_data.shape[-1]) #use 1% of original amount of data\n",
    "        n_hidden = 5 #number of hidden layers \n",
    "        NN_width = 1024*4  #width of the hidden layers\n",
    "        error = nn.L1Loss()\n",
    "        net = AE.AutoEncoder(im_size, n_latent, n_hidden, NN_width, taper = True, square=False).to(device)\n",
    "\n",
    "        if opt == 'SGD':\n",
    "            optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "        elif opt == 'SGD_momentum':\n",
    "            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "        elif opt == 'Adam':\n",
    "            optimizer = torch.optim.Adam(net.parameters(), betas=(0.9, 0.999), lr=lr)\n",
    "        elif opt == 'Adam_beta0.5':\n",
    "            optimizer = torch.optim.Adam(net.parameters(), betas=(0.5, 0.999), lr=lr)\n",
    "        elif opt == 'RMSprop':\n",
    "            optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)\n",
    "        elif opt == 'LBFGS':\n",
    "            optimizer = torch.optim.LBFGS(net.parameters(), lr=lr)\n",
    "        elif opt == 'Adagrad':\n",
    "            optimizer = torch.optim.Adagrad(net.parameters(), lr=lr)\n",
    "\n",
    "        #train\n",
    "        train_out = AE.train_cyl(net, optimizer, dataloader, dataloader_test, epochs=EPOCHS)\n",
    "\n",
    "        #save results\n",
    "        out_fname = out_dir + 'train_results_{}_lr={}.png'.format(train_out['optimizer'], lr)\n",
    "        AE.plot_cyl_reconst(out_fname, train_out['net'], dataloader_test, Nx, Ny, grid_x, grid_y)\n",
    "\n",
    "        #plot loss curve \n",
    "        plt.figure(figsize = (8,6))\n",
    "        plt.plot(train_out['losses'])\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('L1 Loss')\n",
    "        plt.yscale('log')\n",
    "        plt.savefig(out_dir + 'loss_curve_{}_lr={}.png'.format(train_out['optimizer'], lr))\n",
    "        plt.close()\n",
    "\n",
    "        #save losses\n",
    "        np.save(out_dir + 'losses_{}_lr={}.npy'.format(train_out['optimizer'], lr), train_out['losses'])\n",
    "\n",
    "        val_losses[opt + '_' + str(lr)] = train_out['test_error']\n",
    "\n",
    "        #empty cuda cache\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adam_0.0001': 0.008726282852391402,\n",
       " 'Adam_0.001': 0.011560090600202482,\n",
       " 'Adagrad_0.001': 0.011854360997676849,\n",
       " 'Adam_beta0.5_0.0001': 0.01714270965506633,\n",
       " 'RMSprop_0.0001': 0.021597522621353466,\n",
       " 'Adam_1e-05': 0.10366777380307515,\n",
       " 'Adam_beta0.5_1e-05': 0.1042220413684845,\n",
       " 'RMSprop_1e-05': 0.10497997353474299,\n",
       " 'Adagrad_0.0001': 0.10590006858110428,\n",
       " 'RMSprop_0.001': 0.13626183867454528,\n",
       " 'Adagrad_1e-05': 0.13760856886704761,\n",
       " 'SGD_momentum_0.01': 0.14520154595375062,\n",
       " 'SGD_0.01': 0.14824874103069305,\n",
       " 'SGD_momentum_0.001': 0.14825064142545064,\n",
       " 'SGD_0.001': 0.14932768940925598,\n",
       " 'SGD_momentum_0.0001': 0.1493283063173294,\n",
       " 'SGD_0.0001': 0.14947597483793895,\n",
       " 'SGD_momentum_1e-05': 0.14947610398133596,\n",
       " 'SGD_1e-05': 0.14949079950650532,\n",
       " 'Adagrad_0.01': 0.3035383433103561,\n",
       " 'Adam_beta0.5_0.01': 0.3536389301220576,\n",
       " 'Adam_beta0.5_0.001': 12.834009416898091,\n",
       " 'Adam_0.01': 1547545.9333333333,\n",
       " 'RMSprop_0.01': 329855612791.4667}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print sorted val losses dictionary\n",
    "sorted_val_losses = {k: v for k, v in sorted(val_losses.items(), key=lambda item: item[1])}\n",
    "np.save(SAVE_DIR+'val_losses.npy', sorted_val_losses) #np.load(SAVE_DIR+'val_losses.npy', allow_pickle=True).item()\n",
    "sorted_val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
