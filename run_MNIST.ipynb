{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dstre\\AppData\\Local\\Temp\\ipykernel_49384\\2255066242.py:6: UserWarning: Treat the new Tool classes introduced in v1.5 as experimental for now; the API and rcParam may change in future versions.\n",
      "  import matplotlib.pyplot as plt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import Autoencoder as AE\n",
    "import os\n",
    "\n",
    "# use GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Reproducibility  \n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data ### \n",
    "\n",
    "#Use create a transform that uses bilinear interpolation to resize images to 64 square\n",
    "im_size = 64\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(im_size),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#load dataset and init dataloader (THIS WILL DOWNLOAD THE DATASET)\n",
    "#dataloader will automatically batch and handle data\n",
    "#The MNIST dataset is \"premade,\" we will likely need to use TensorDataset to load our own datasets\n",
    "mnist_dset_train = torchvision.datasets.MNIST(root = './data/', download=True, train = True, transform = transform)\n",
    "batch_size = 128\n",
    "dataloader = torch.utils.data.DataLoader(mnist_dset_train, shuffle=True, batch_size=batch_size) \n",
    "\n",
    "\n",
    "#Create test dataset and load into dataloader\n",
    "mnist_dset_test = torchvision.datasets.MNIST(root = './data/MNIST/', download=True, train = False, transform = transform)\n",
    "num_samples = 500\n",
    "dataloader_test = torch.utils.data.DataLoader(mnist_dset_test, shuffle=True, batch_size=num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with optimizer: SGD    lr = 0.001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10676886141300201\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.10398663580417633\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.10393942892551422\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.11133380234241486\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.10645522177219391\n",
      "\n",
      "Training with optimizer: SGD    lr = 0.001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.10405895859003067\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.10415985435247421\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.11142878234386444\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.10662300139665604\n",
      "\n",
      "Training with optimizer: SGD    lr = 0.0001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.10415934026241302\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.10435932874679565\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.1117313951253891\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.10701459646224976\n",
      "\n",
      "Training with optimizer: SGD    lr = 1e-05\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.10416924953460693\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.10437928140163422\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.11176206171512604\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.10705478489398956\n",
      "\n",
      "Training with optimizer: SGD_momentum    lr = 0.001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.10318617522716522\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.10255388915538788\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.10911960154771805\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.10377705097198486\n",
      "\n",
      "Training with optimizer: SGD_momentum    lr = 0.001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.10318617522716522\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.10255388915538788\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.10911960154771805\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.10377705097198486\n",
      "\n",
      "Training with optimizer: SGD_momentum    lr = 0.0001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.1040598452091217\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.10416068881750107\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.1114296019077301\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.10662378370761871\n",
      "\n",
      "Training with optimizer: SGD_momentum    lr = 1e-05\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.10415943711996078\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.10435943305492401\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.11173150688409805\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.1070147156715393\n",
      "\n",
      "Training with optimizer: Adam    lr = 0.001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.030070748180150986\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.02516503818333149\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.0245395265519619\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.02402854524552822\n",
      "\n",
      "Training with optimizer: Adam    lr = 0.001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.030070748180150986\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.02516503818333149\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.0245395265519619\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.02402854524552822\n",
      "\n",
      "Training with optimizer: Adam    lr = 0.0001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.04935753345489502\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.04475884884595871\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.048206672072410583\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.04545113444328308\n",
      "\n",
      "Training with optimizer: Adam    lr = 1e-05\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.06852628290653229\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.05291488766670227\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.05708886682987213\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.05461473762989044\n",
      "\n",
      "Training with optimizer: Adam_beta0.5    lr = 0.001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.03244633227586746\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.024806492030620575\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.0235186405479908\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.023322220891714096\n",
      "\n",
      "Training with optimizer: Adam_beta0.5    lr = 0.001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.03244633227586746\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.024806492030620575\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.0235186405479908\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.023322220891714096\n",
      "\n",
      "Training with optimizer: Adam_beta0.5    lr = 0.0001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.04790160432457924\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.04478740692138672\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.04812643676996231\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.045565348118543625\n",
      "\n",
      "Training with optimizer: Adam_beta0.5    lr = 1e-05\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.06424973160028458\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.05282510444521904\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.05707245692610741\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.05460887774825096\n",
      "\n",
      "Training with optimizer: RMSprop    lr = 0.001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.03318164497613907\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.02714516595005989\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.02489934116601944\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.023678291589021683\n",
      "\n",
      "Training with optimizer: RMSprop    lr = 0.001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.03318164497613907\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.02714516595005989\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.02489934116601944\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.023678291589021683\n",
      "\n",
      "Training with optimizer: RMSprop    lr = 0.0001\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.04991485923528671\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.045844074338674545\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.04850086569786072\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.04523197188973427\n",
      "\n",
      "Training with optimizer: RMSprop    lr = 1e-05\n",
      "Epoch: 0/10     Iter: 0     Loss: 0.10695052146911621\n",
      "Epoch: 2/10     Iter: 1000     Loss: 0.07619252800941467\n",
      "Epoch: 4/10     Iter: 2000     Loss: 0.053742118179798126\n",
      "Epoch: 6/10     Iter: 3000     Loss: 0.057029590010643005\n",
      "Epoch: 8/10     Iter: 4000     Loss: 0.054452113807201385\n"
     ]
    }
   ],
   "source": [
    "optimizers = [\n",
    "    'SGD',\n",
    "    'SGD_momentum',\n",
    "    'Adam',\n",
    "    'Adam_beta0.5',\n",
    "    'RMSprop'\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "lrs = [1e-3, 1e-3, 1e-4, 1e-5]\n",
    "EPOCHS = 10\n",
    "SAVE_DIR = './train_results_MNIST/trial1/'\n",
    "\n",
    "val_losses = {}\n",
    "for opt in optimizers:\n",
    "    for lr in lrs:\n",
    "        #create directory for results\n",
    "        out_dir = SAVE_DIR + opt + '/'\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        print('\\nTraining with optimizer: {}    lr = {}'.format(opt, lr))\n",
    "        #Init network\n",
    "        n_latent = int(0.01* 28**2) #use 1% of original amount of data\n",
    "        n_hidden = 5 #number of hidden layers \n",
    "        NN_width = 1024*4  #width of the hidden layers\n",
    "        error = nn.MSELoss()\n",
    "        net = AE.AutoEncoder(im_size**2, n_latent, n_hidden, NN_width, taper = True, square=True).to(device)\n",
    "\n",
    "        if opt == 'SGD':\n",
    "            optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "        elif opt == 'SGD_momentum':\n",
    "            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "        elif opt == 'Adam':\n",
    "            optimizer = torch.optim.Adam(net.parameters(), betas=(0.9, 0.999), lr=lr)\n",
    "        elif opt == 'Adam_beta0.5':\n",
    "            optimizer = torch.optim.Adam(net.parameters(), betas=(0.5, 0.999), lr=lr)\n",
    "        elif opt == 'RMSprop':\n",
    "            optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)\n",
    "        elif opt == 'LBFGS':\n",
    "            optimizer = torch.optim.LBFGS(net.parameters(), lr=lr)\n",
    "\n",
    "        #train\n",
    "        train_out = AE.train_mnist(net, optimizer, dataloader, dataloader_test, epochs=EPOCHS, error = error)\n",
    "\n",
    "        #save results\n",
    "        out_fname = out_dir + 'train_results_{}_lr={}.png'.format(train_out['optimizer'], lr)\n",
    "        AE.plot_mnist_reconst(out_fname, train_out['net'], dataloader_test)\n",
    "\n",
    "        #plot loss curve \n",
    "        plt.figure(figsize = (8,6))\n",
    "        plt.plot(train_out['losses'])\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.yscale('log')\n",
    "        plt.savefig(out_dir + 'loss_curve_{}_lr={}.png'.format(train_out['optimizer'], lr))\n",
    "        plt.close()\n",
    "\n",
    "        #save losses\n",
    "        np.save(out_dir + 'losses_{}_lr={}.npy'.format(train_out['optimizer'], lr), train_out['losses'])\n",
    "\n",
    "        val_losses[opt + '_' + str(lr)] = train_out['test_error']\n",
    "\n",
    "        #empty cuda cache\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adam_0.001': 0.021386262401938437,\n",
       " 'Adam_beta0.5_0.001': 0.021894904691725968,\n",
       " 'RMSprop_0.001': 0.02386805769056082,\n",
       " 'Adam_beta0.5_0.0001': 0.04258222114294767,\n",
       " 'RMSprop_0.0001': 0.04259118717163801,\n",
       " 'Adam_0.0001': 0.04299421682953834,\n",
       " 'RMSprop_1e-05': 0.05262085497379303,\n",
       " 'Adam_beta0.5_1e-05': 0.052843798883259296,\n",
       " 'Adam_1e-05': 0.05284702740609646,\n",
       " 'SGD_momentum_0.001': 0.1035735435783863,\n",
       " 'SGD_0.001': 0.10681958533823491,\n",
       " 'SGD_momentum_0.0001': 0.106820347905159,\n",
       " 'SGD_0.0001': 0.10727736800909042,\n",
       " 'SGD_momentum_1e-05': 0.10727750062942505,\n",
       " 'SGD_1e-05': 0.10732478089630604}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print sorted val losses dictionary\n",
    "sorted_val_losses = {k: v for k, v in sorted(val_losses.items(), key=lambda item: item[1])}\n",
    "np.save(SAVE_DIR+'val_losses.npy', sorted_val_losses) #np.load(SAVE_DIR+'val_losses.npy', allow_pickle=True).item()\n",
    "sorted_val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
