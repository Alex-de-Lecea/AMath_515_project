{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import Autoencoder as AE\n",
    "import scipy\n",
    "import scipy.interpolate\n",
    "import os \n",
    "\n",
    "# use GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Reproducibility  \n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and Process Data ###\n",
    "run_interp = False \n",
    "cyl_data = np.load('./SamsStuff/data_numpy.npy') # of shape [n_samples, (u, v, vort), n_gridpoints]\n",
    "xy_coords = np.load('./SamsStuff/xy_coords.npy').T # of shape [n_gridpoints, (x, y)].T\n",
    "vort_data = cyl_data[:, 2, :]\n",
    "\n",
    "# interpolate onto structured grid\n",
    "x = xy_coords[0, :]\n",
    "y = xy_coords[1, :]\n",
    "x_bounds = [x.min(), x.max()]\n",
    "y_bounds = [y.min(), y.max()]\n",
    "scaling = 20 #controls resolution of structured grid\n",
    "Nx = int((x_bounds[1] - x_bounds[0]) * scaling)\n",
    "Ny = int((y_bounds[1] - y_bounds[0]) * scaling)\n",
    "grid_x, grid_y = np.meshgrid(np.linspace(x_bounds[0], x_bounds[1], Nx), np.linspace(y_bounds[0], y_bounds[1], Ny))\n",
    "\n",
    "# cylinder mask \n",
    "mask = np.ones_like(grid_x)\n",
    "mask[np.where(grid_x**2 + grid_y**2 < 0.5**2)] = 0\n",
    "\n",
    "if run_interp: # takes ~3m for scaling = 20\n",
    "    vort_structured = np.zeros((vort_data.shape[0], Ny, Nx))\n",
    "    for i in range(vort_data.shape[0]):\n",
    "        interpolated = scipy.interpolate.griddata((x,y), vort_data[i], (grid_x, grid_y), method='cubic')\n",
    "        vort_structured[i] = interpolated * mask\n",
    "    np.save('vort_structured{}.npy'.format(scaling), vort_structured)\n",
    "else:\n",
    "    vort_structured = np.load('vort_structured{}.npy'.format(scaling))\n",
    "\n",
    "im_size = vort_structured.shape[1] * vort_structured.shape[2]\n",
    "\n",
    "### Dataloaders ###\n",
    "test_perc = 0.1\n",
    "split_idx = int((1-test_perc) * vort_structured.shape[1])\n",
    "\n",
    "batch_size = 128\n",
    "vort_torch = torch.from_numpy(vort_structured).float()\n",
    "vort_dset = torch.utils.data.TensorDataset(vort_torch[split_idx:, :, :])\n",
    "dataloader = torch.utils.data.DataLoader(vort_dset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "vort_dset_test = torch.utils.data.TensorDataset(vort_torch[:split_idx, :, :])\n",
    "dataloader_test = torch.utils.data.DataLoader(vort_dset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizers import GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\n",
    "    'Adam',\n",
    "    #'Adam_beta0.5',\n",
    "    #GD\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "lr_schedulers = [\n",
    "    #'CosineAnnealingWarmRestarts', \n",
    "    #'ReduceLROnPlateau', \n",
    "    'Armijo'\n",
    "]\n",
    "EPOCHS = 2\n",
    "initial_lr = 0.001\n",
    "armijo = False\n",
    "SAVE_DIR = './train_results_cyl/trial4/'\n",
    "\n",
    "val_losses = {}\n",
    "for opt in optimizers:\n",
    "    for sched in lr_schedulers:\n",
    "        #create directory for results\n",
    "        out_dir = SAVE_DIR + opt + '/'\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        print('\\nTraining with optimizer: {}    lr = {}'.format(opt, sched))\n",
    "        #Init network\n",
    "        n_latent = int(0.01* cyl_data.shape[-1]) #use 1% of original amount of data\n",
    "        n_hidden = 5 #number of hidden layers \n",
    "        NN_width = 1024*4  #width of the hidden layers\n",
    "        error = nn.L1Loss()\n",
    "        net = AE.AutoEncoder(im_size, n_latent, n_hidden, NN_width, taper = True, square=False).to(device)\n",
    "\n",
    "        if opt == 'GD':\n",
    "            optimizer = torch.optim.SGD(net.parameters(), lr=initial_lr)\n",
    "        elif opt == 'Adam':\n",
    "            optimizer = torch.optim.Adam(net.parameters(), betas=(0.9, 0.999), lr=initial_lr)\n",
    "        elif opt == 'Adam_beta0.5':\n",
    "            optimizer = torch.optim.Adam(net.parameters(), betas=(0.5, 0.999), lr=initial_lr)\n",
    "\n",
    "        if sched == 'CosineAnnealingWarmRestarts':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 200)\n",
    "            use_loss = False\n",
    "        elif sched == 'ReduceLROnPlateau':\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience  = 7)\n",
    "            use_loss = True\n",
    "        elif sched == 'Armijo':\n",
    "            scheduler = None\n",
    "            use_loss = False\n",
    "            armijo = True\n",
    "\n",
    "\n",
    "        #train\n",
    "        train_out = AE.train_cyl(net, optimizer, dataloader, dataloader_test, epochs=EPOCHS, scheduler = scheduler, armijo = armijo, use_loss = use_loss)\n",
    "\n",
    "        #save results\n",
    "        out_fname = out_dir + 'train_results_{}_lr={}.png'.format(train_out['optimizer'], sched)\n",
    "        AE.plot_cyl_reconst(out_fname, train_out['net'], dataloader_test, Nx, Ny, grid_x, grid_y)\n",
    "\n",
    "        #plot loss curve \n",
    "        plt.figure(figsize = (8,6))\n",
    "        plt.plot(train_out['losses'])\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('L1 Loss')\n",
    "        plt.yscale('log')\n",
    "        plt.savefig(out_dir + 'loss_curve_{}_lr={}.png'.format(train_out['optimizer'], sched))\n",
    "        plt.close()\n",
    "\n",
    "        #save losses\n",
    "        np.save(out_dir + 'losses_{}_lr={}.npy'.format(train_out['optimizer'], sched), train_out['losses'])\n",
    "\n",
    "        val_losses[opt + '_' + str(sched)] = train_out['test_error']\n",
    "\n",
    "        #empty cuda cache\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sorted val losses dictionary\n",
    "sorted_val_losses = {k: v for k, v in sorted(val_losses.items(), key=lambda item: item[1])}\n",
    "np.save(SAVE_DIR+'val_losses.npy', sorted_val_losses) #np.load(SAVE_DIR+'val_losses.npy', allow_pickle=True).item()\n",
    "sorted_val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
